{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b98bc",
   "metadata": {},
   "source": [
    "### md5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af4ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return md5(f.read()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd09551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6fe10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'\\\\images')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir()\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, os\n",
    "duplicates = []\n",
    "hash_keys = dict()\n",
    "for index, filename in  enumerate(os.listdir('.')):\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            filehash = hashlib.md5(f.read()).hexdigest()\n",
    "        if filehash not in hash_keys: \n",
    "            hash_keys[filehash] = index\n",
    "        else:\n",
    "            duplicates.append((index,hash_keys[filehash]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b5adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8d91b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file_indexes in duplicates[:30]:\n",
    "    try:\n",
    "    \n",
    "        plt.subplot(121),plt.imshow(imread(file_list[file_indexes[1]]))\n",
    "        plt.title(file_indexes[1]), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "        plt.subplot(122),plt.imshow(imread(file_list[file_indexes[0]]))\n",
    "        plt.title(str(file_indexes[0])), plt.xticks([]), plt.yticks([])\n",
    "        plt.show()\n",
    "    \n",
    "    except OSError as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "\n",
    "for index in duplicates:\n",
    "    os.remove(file_list[index[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5c351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a420ffa3",
   "metadata": {},
   "source": [
    "There are four types of Hashes:-\n",
    "\n",
    "1) Average Hash (aHash): This algorithm computes the average brightness of an image and then reduces the image to a binary hash by comparing each pixel's brightness to the average. It's simple and fast, but not very robust to scaling, rotation, or other image transformations.\n",
    "\n",
    "2) Difference Hash (dHash): This algorithm works similarly to aHash, but instead of using the average brightness, it computes the difference between adjacent pixels. The resulting hash is more robust to certain types of image transformations, but less so to others.\n",
    "\n",
    "3) Perceptual Hash (pHash): This algorithm uses a more complex approach based on the Discrete Cosine Transform (DCT) to compute a hash that is more invariant to image transformations like scaling, rotation, and brightness changes. It's generally considered to be more robust than aHash or dHash, but also slower.\n",
    "\n",
    "4) Wavelet Hash (wHash): This algorithm uses a multi-resolution wavelet decomposition to compute a hash that is invariant to certain types of image transformations, including scaling, rotation, and translation. It's more complex than aHash or dHash, but generally faster than pHash.\n",
    "\n",
    "Each of these algorithms has its own strengths and weaknesses, and which one you choose will depend on the specific requirements of your application. In general, if you need a fast and simple hash for images that won't be heavily transformed, aHash or dHash may be sufficient. If you need a more robust hash that can handle a wider range of transformations, pHash or wHash may be more appropriate.\n",
    "\n",
    "Let's try pHash for our use:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bec673",
   "metadata": {},
   "source": [
    "### pHASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d974d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "from PIL import Image \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65ba6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "duplicates = []\n",
    "hash_keys = dict()\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# add 'duplicate_url' column to the dataframe\n",
    "df['duplicate_url'] = \"\"\n",
    "\n",
    "# create a new dataframe for duplicate images\n",
    "dup_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# set a threshold for perceptual hash similarity scores\n",
    "threshold = 5\n",
    "\n",
    "for index, url in enumerate(df['url']):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # compute perceptual hash of the image\n",
    "            img = Image.open(io.BytesIO(response.content))\n",
    "            phash = str(imagehash.phash(img))\n",
    "            \n",
    "            # compare the hash to identify duplicates\n",
    "            if phash not in hash_keys:\n",
    "                hash_keys[phash] = index\n",
    "            else:\n",
    "                dup_index = hash_keys[phash]\n",
    "                score = imagehash.hex_to_hash(phash) - imagehash.hex_to_hash(str(imagehash.phash(Image.open(io.BytesIO(requests.get(df['url'][dup_index]).content)))))\n",
    "\n",
    "                # check if the score is below the threshold\n",
    "                if score <= threshold:\n",
    "                    duplicates.append((index, dup_index))\n",
    "                    # add duplicate image to duplicate dataframe\n",
    "                    dup_df = pd.concat([dup_df, df.iloc[[index]]])\n",
    "\n",
    "                    # update 'duplicates' column of original dataframe with URLs of all duplicate images\n",
    "                    dup_urls = [df.at[dup_index, 'url'], url]\n",
    "                    duplicate_urls = [url for url in dup_urls if url != df.at[index, 'url']]\n",
    "                    duplicate_ids = [df.at[dup_index, 'item_id'] for url in dup_urls if url != df.at[index, 'url']]\n",
    "                    df.at[index, 'duplicate_url'] = ', '.join(duplicate_urls)\n",
    "                    df.at[index, 'duplicate_ids'] = ', '.join(map(str, duplicate_ids))\n",
    "                    \n",
    "                    # add 'duplicate_category_name' column to indicate the category of the duplicate image\n",
    "                    duplicate_category = df.at[dup_index, 'category_name']\n",
    "                    df.at[index, 'duplicate_category_name'] = duplicate_category\n",
    "\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Error: Could not download image at index {index} ({url}) (status code: {response.status_code})\")\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: Could not download image at index {index} ({url}) ({str(e)})\")\n",
    "\n",
    "# concatenate original and duplicate dataframes\n",
    "result_df = pd.concat([df, dup_df])   \n",
    "\n",
    "# print the duplicate images that belong to the different classes\n",
    "for dup in duplicates:\n",
    "    if df.at[dup[0], 'category_name'] != df.at[dup[1], 'category_name']:\n",
    "        print(f\"Duplicate images with item_id {df.at[dup[0], 'item_id']} and {df.at[dup[1], 'item_id']} belong to different classes ({df.at[dup[0], 'category_name']} and {df.at[dup[1], 'category_name']})\")\n",
    "        \n",
    "result_df.to_csv(\"duplicate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c59fb5e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create sample dataframe\n",
    "df = pd.read_csv(\"duplicate.csv\")\n",
    "\n",
    "# strip leading and trailing white spaces and convert to lowercase\n",
    "df['category_name'] = df['category_name'].str.strip().str.lower()\n",
    "df['duplicate_category_name'] = df['duplicate_category_name'].str.strip().str.lower()\n",
    "\n",
    "# create comparison column\n",
    "df['comparison'] = 'no'\n",
    "df.loc[df['category_name'] == df['duplicate_category_name'], 'comparison'] = 'yes'\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
